{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf9121e7",
   "metadata": {},
   "source": [
    "In this Notebook we create and sketch tools for the Agent. Building proofs of concept (PoC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29d8861",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94d01490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f09b59ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f32d7959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Whisper\n",
    "\n",
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e131840",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Langfuse client is disabled since no public_key was provided as a parameter or environment variable 'LANGFUSE_PUBLIC_KEY'. See our docs: https://langfuse.com/docs/sdk/python/low-level-sdk#initialize-client\n"
     ]
    }
   ],
   "source": [
    "# Import Agent\n",
    "\n",
    "os.sys.path.append(\"../src\")\n",
    "os.sys.path.append(\"../src/agents\")\n",
    "\n",
    "import react  # My AI assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "248cf6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.sys.path.append(\"../src/utils\")\n",
    "\n",
    "from gaia_eval import get_agent_response, evaluate_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ca7a88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santiagoal/.cache/pypoetry/virtualenvs/chappie-CHLGiFC_-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Loading a dataset cached in a LocalFileSystem is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m gaia_questions \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgaia-benchmark/GAIA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2023_level1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m gaia_questions \u001b[38;5;241m=\u001b[39m gaia_questions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Filter for dev purposes\u001b[39;00m\n\u001b[1;32m      7\u001b[0m gaia_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(gaia_questions)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/chappie-CHLGiFC_-py3.12/lib/python3.12/site-packages/datasets/load.py:2166\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2163\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2164\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2165\u001b[0m )\n\u001b[0;32m-> 2166\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2167\u001b[0m \u001b[38;5;66;03m# Rename and cast features to match task schema\u001b[39;00m\n\u001b[1;32m   2168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2169\u001b[0m     \u001b[38;5;66;03m# To avoid issuing the same warning twice\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/chappie-CHLGiFC_-py3.12/lib/python3.12/site-packages/datasets/builder.py:1173\u001b[0m, in \u001b[0;36mDatasetBuilder.as_dataset\u001b[0;34m(self, split, run_post_process, verification_mode, ignore_verifications, in_memory)\u001b[0m\n\u001b[1;32m   1171\u001b[0m is_local \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m is_remote_filesystem(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fs)\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local:\n\u001b[0;32m-> 1173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading a dataset cached in a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fs)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_dir):\n\u001b[1;32m   1175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1176\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: could not find data in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please make sure to call \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1177\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilder.download_and_prepare(), or use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1178\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets.load_dataset() before trying to access the Dataset object.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1179\u001b[0m     )\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Loading a dataset cached in a LocalFileSystem is not supported."
     ]
    }
   ],
   "source": [
    "\n",
    "# Import GAIA Questions\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "gaia_questions = load_dataset(path=\"gaia-benchmark/GAIA\", name=\"2023_level1\")\n",
    "gaia_questions = gaia_questions[\"validation\"]  # Filter for dev purposes\n",
    "gaia_df = pd.DataFrame(gaia_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c08c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaia_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d34bca",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0150df47",
   "metadata": {},
   "outputs": [],
   "source": [
    "filetypes = {d[1].file_path.split(\".\")[-1] for d in gaia_df.iterrows()}\n",
    "filetypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25a6fd5",
   "metadata": {},
   "source": [
    "# Image Handler Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4ce0bc",
   "metadata": {},
   "source": [
    "### Identify Image-like tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "387e0aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_filetypes = ('png', 'jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ec59bb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gaia_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m image_tasks_df \u001b[38;5;241m=\u001b[39m \u001b[43mgaia_df\u001b[49m[gaia_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: row\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m img_filetypes)]\n\u001b[1;32m      2\u001b[0m image_tasks_df\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gaia_df' is not defined"
     ]
    }
   ],
   "source": [
    "image_tasks_df = gaia_df[gaia_df[\"file_path\"].apply(lambda row: row.split(\".\")[-1] in img_filetypes)]\n",
    "image_tasks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180294ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tasks_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672fec32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c16f891",
   "metadata": {},
   "source": [
    "### Tool POC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1091615",
   "metadata": {},
   "source": [
    "#### (1/2) Donut for OCR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d0a8cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79ddc685",
   "metadata": {},
   "source": [
    "#### (2/2) YoLo for Object Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3467ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6866d6d",
   "metadata": {},
   "source": [
    "### Integrate Tool and Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb541af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54fadcf2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cbc05d",
   "metadata": {},
   "source": [
    "## Audio Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b579ef",
   "metadata": {},
   "source": [
    "Let's study where does our current Agent fail, especially on audio-like questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7a1444",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d3af35",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaia_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9e932b",
   "metadata": {},
   "source": [
    "### Identify Audio-like tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8c9a12",
   "metadata": {},
   "source": [
    "The first questions is to understand which questions need audio? \n",
    "\n",
    "* Hypothesis: Just look at such questions that have a .mp3 file attached\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23516201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather a sample file from any task\n",
    "\n",
    "audio_tasks = gaia_df[(gaia_df[\"file_path\"].str.len()>0) & (gaia_df[\"file_path\"].str.endswith(\".mp3\"))]\n",
    "sample_task = audio_tasks.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa03a8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{(audio_tasks.shape[0] / gaia_df.shape[0]) * 100:.1f}% of tasks need audio processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30a0394",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47c9bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sample file\n",
    "\n",
    "filepath = sample_task[\"file_path\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d07cab8",
   "metadata": {},
   "source": [
    "Let's import the mp3 file with [ffmpeg](https://stackoverflow.com/questions/9458480/read-mp3-in-python-3#:~:text=%24%20ffmpeg%20%2Di%20foo.mp3%20%2Dvn%20%2Dacodec%20pcm_s16le%20%2Dac%201%20%2Dar%2044100%20%2Df%20wav%20foo.wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82f6246",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data_path = \"/home/santiagoal/current-projects/chappie/data/temp-data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e255c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run:\n",
    "    os.system(f\"ffmpeg -i {filepath} -vn -acodec pcm_s16le -ac 1 -ar 44100 -f wav {temp_data_path}sample_audio.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9d0ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run:\n",
    "    use_gpu = False\n",
    "    model_size = \"tiny\"\n",
    "\n",
    "    model = (\n",
    "        whisper.load_model(model_size).cuda()\n",
    "        if use_gpu\n",
    "        else whisper.load_model(model_size)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df808df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce2c352",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run: \n",
    "    raw_transcript = model.transcribe(\n",
    "            filepath,\n",
    "            word_timestamps=False,\n",
    "            no_speech_threshold=0.5,\n",
    "            condition_on_previous_text=True,\n",
    "            compression_ratio_threshold=2.0,\n",
    "        )\n",
    "\n",
    "    transcript = raw_transcript[\"text\"]\n",
    "    print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0654ee48",
   "metadata": {},
   "source": [
    "### Tool POC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83335239",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if run: \n",
    "    def transcriber(audio_path: str, ai_model = model) -> str:\n",
    "        \"\"\"\n",
    "        Transcribes an audio file\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        audio_path : str\n",
    "            Path to the audio file\n",
    "        ai_model\n",
    "            audio-to-text AI model \n",
    "\n",
    "        Returns:\n",
    "            str: Text of the transcript \n",
    "        \"\"\"\n",
    "        raw_transcript = ai_model.transcribe(\n",
    "            audio_path,\n",
    "            word_timestamps=False,\n",
    "            no_speech_threshold=0.5,\n",
    "            condition_on_previous_text=True,\n",
    "            compression_ratio_threshold=2.0,\n",
    "        )\n",
    "\n",
    "        transcript = raw_transcript[\"text\"]\n",
    "\n",
    "        return transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9d4e58",
   "metadata": {},
   "source": [
    "### Integrate Tool POC and experiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dee4276",
   "metadata": {},
   "source": [
    "The changes have been integrated, now we will experiment with the new version of the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811185e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_tasks[\"Agent response\"] = audio_tasks.apply(func=get_agent_response, axis=1)\n",
    "audio_tasks[\"is_correct\"] = audio_tasks.apply(func=eval_answer, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ab6ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e9241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next steps: Update model... "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chappie-CHLGiFC_-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
